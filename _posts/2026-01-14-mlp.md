---
title: "Bài 1: Giới thiệu về MLP (Multi-Layer Perceptron)"
date: 2026-01-14
categories:
  - Machine Learning
tags:
  - MLP
  - Deep Learning
toc: true
---

# Tổng quan về Multilayer Perceptrons (Mạng Perceptron Đa lớp)

**Nguồn tham khảo:** [Dive into Deep Learning - Chapter 5](https://d2l.ai/chapter_multilayer-perceptrons/index.html)

Chương này giới thiệu về các mô hình mạng nơ-ron sâu thực sự đầu tiên, vượt qua các giới hạn của mô hình tuyến tính bằng cách giới thiệu các lớp ẩn (hidden layers) và tính phi tuyến tính. Dưới đây là hệ thống các nội dung cốt lõi.

---

## 1. Multilayer Perceptrons (MLP) là gì?

Các mô hình tuyến tính (như Linear Regression hay Softmax Regression) thường gặp khó khăn với dữ liệu phức tạp không thể phân tách tuyến tính. **Multilayer Perceptrons (MLP)** giải quyết vấn đề này bằng cách xếp chồng nhiều lớp nơ-ron lên nhau, để học được cả mối quan hệ tuyến tính và phi tuyến tính.

### Kiến trúc cơ bản
* **Input Layer (Lớp đầu vào):** Nhận dữ liệu đặc trưng.
* **Hidden Layers (Lớp ẩn):** Nằm giữa đầu vào và đầu ra. Đây là nơi diễn ra các tính toán trung gian. Một mạng có thể có nhiều lớp ẩn, tạo nên độ "sâu" (Deep Learning).
* **Output Layer (Lớp đầu ra):** Trả về kết quả dự đoán cuối cùng (số thực cho hồi quy hoặc xác suất cho phân loại).

### Tính phi tuyến (Non-linearity)
Nếu chỉ xếp chồng các lớp tuyến tính ($Wx + b$), toàn bộ mạng vẫn chỉ tương đương với một mô hình tuyến tính đơn lẻ. 

$$\mathbf{O} = (\mathbf{X}\mathbf{W}^{(1)} + \mathbf{b}^{(1)})\mathbf{W}^{(2)} + \mathbf{b}^{(2)} = \mathbf{X}\mathbf{W}^{(1)}\mathbf{W}^{(2)} + \mathbf{b}^{(1)}\mathbf{W}^{(2)} + \mathbf{b}^{(2)} = \mathbf{X}\mathbf{W} + \mathbf{b}$$

Để mạng học được các đặc trưng phức tạp, chúng ta cần áp dụng **Hàm kích hoạt (Activation Function)** phi tuyến tính sau mỗi lớp ẩn.

**Extention: Kernel Trick**

Đây là một hướng tiếp cận có phần đối lập với MLP. Thay vì tập trung vào chiều sâu, thì Kernel Trick tập trung vào chiều rộng, bằng việc đưa các W về tích vô hướng vector, sau đó dùng các hàm kernel để thực hiện tính toán tương đương trên không gian lớn hơn. Từ đó, giúp mô hình có khả năng học hiểu các quan hệ phi tuyến chỉ với tính toán ở không gian ít chiều.

Một số hàm kernel thông dụng:

* **Polynomial:**$$K(\mathbf{x}, \mathbf{y}) = (\gamma \mathbf{x}^T \mathbf{y} + r)^d$$ (d chiều)

* **RBF:**$$K(\mathbf{x}, \mathbf{y}) = \exp(-\gamma ||\mathbf{x} - \mathbf{y}||^2)$$ (vô hạn chiều) 

* **Sigmoid:**$$K(\mathbf{x}, \mathbf{y}) = \tanh(\gamma \mathbf{x}^T \mathbf{y} + r)$$

---

## 2. Các hàm kích hoạt (Activation Functions) phổ biến

Hàm kích hoạt quyết định một nơ-ron có "phát hỏa" hay không và phá vỡ tính tuyến tính của mô hình.

### a. ReLU (Rectified Linear Unit)
Đây là hàm phổ biến nhất hiện nay do tính toán đơn giản và hiệu quả.
$$\operatorname{ReLU}(x) = \max(x, 0)$$
* **Ưu điểm:** Giảm thiểu vấn đề biến mất gradient (vanishing gradient), tính toán nhanh.

### b. Sigmoid
Chuyển đổi đầu vào thành giá trị trong khoảng $(0, 1)$.
$$\operatorname{sigmoid}(x) = \frac{1}{1 + \exp(-x)}$$
* **Nhược điểm:** Dễ gặp vấn đề biến mất gradient khi giá trị đầu vào quá lớn hoặc quá nhỏ. Thường chỉ dùng ở lớp đầu ra cho bài toán nhị phân.

### c. Tanh (Hyperbolic Tangent)
Tương tự Sigmoid nhưng đưa giá trị về khoảng $(-1, 1)$, giúp dữ liệu tập trung quanh 0 (zero-centered).
$$\tanh(x) = \frac{1 - \exp(-2x)}{1 + \exp(-2x)}$$

---

## 3. Cài đặt (Implementation)

Chương này hướng dẫn hai cách tiếp cận để xây dựng MLP:
1.  **Từ đầu (From Scratch):** Tự định nghĩa các tham số mô hình (Weights, Biases) và phép toán ma trận, tự viết vòng lặp huấn luyện. Cách này giúp hiểu sâu về cơ chế bên trong.
2.  **Ngắn gọn (Concise):** Sử dụng các API cấp cao của Deep Learning Frameworks (như `torch.nn` trong PyTorch). Chúng ta chỉ cần khai báo các lớp (`nn.Linear`) và hàm kích hoạt (`nn.ReLU`), Framework sẽ tự động quản lý tham số.

---

## 4. Cơ chế huấn luyện: Lan truyền xuôi & Ngược

Để huấn luyện mạng sâu, chúng ta cần hiểu luồng dữ liệu và gradient:

* **Forward Propagation (Lan truyền xuôi):** Dữ liệu đi từ Input $\rightarrow$ Hidden $\rightarrow$ Output để tính toán dự đoán và hàm mất mát (Loss).
* **Computational Graph (Đồ thị tính toán):** Framework dựng một đồ thị để theo dõi các phép toán phục vụ cho việc tính đạo hàm.
* **Backpropagation (Lan truyền ngược):** Tính toán gradient của hàm mất mát theo từng tham số bằng quy tắc chuỗi (chain rule), đi ngược từ Output $\rightarrow$ Input. Đây là thuật toán cốt lõi để cập nhật trọng số.

---

## 5. Ổn định số (Numerical Stability) và Khởi tạo

Huấn luyện mạng sâu rất dễ gặp lỗi do tính chất nhân dồn của gradient qua nhiều lớp:
* **Vanishing Gradients (Biến mất gradient):** Gradient trở nên quá nhỏ (về 0), khiến các lớp đầu không học được gì (thường gặp với Sigmoid/Tanh).
* **Exploding Gradients (Bùng nổ gradient):** Gradient trở nên quá lớn, khiến trọng số cập nhật lung tung, gây lỗi `NaN`.

**Giải pháp:**
* **Khởi tạo tham số (Parameter Initialization):** Không khởi tạo ngẫu nhiên tùy tiện. Các phương pháp như **Xavier Initialization** (cho Sigmoid/Tanh) hoặc **He Initialization** (cho ReLU) giúp giữ phương sai của gradient ổn định giữa các lớp.

---

## 6. Tổng quát hóa (Generalization) & Regularization

Mục tiêu của ML không phải là nhớ dữ liệu huấn luyện, mà là dự đoán tốt trên dữ liệu chưa biết (Test set).

* **Overfitting (Quá khớp):** Mô hình quá phức tạp, học cả nhiễu của tập train $\rightarrow$ Loss train thấp nhưng Loss test cao.
* **Underfitting:** Mô hình quá đơn giản, không học được quy luật dữ liệu.

### Kỹ thuật chính: Dropout
Đây là kỹ thuật regularization đặc trưng cho Neural Networks được giới thiệu trong chương.
* **Cơ chế:** Trong quá trình huấn luyện, ngẫu nhiên "tắt" (cho về 0) một tỷ lệ $p$ các nơ-ron ở lớp ẩn.
* **Tác dụng:** Ngăn các nơ-ron phụ thuộc quá mức vào nhau (co-adaptation), buộc mạng phải học các đặc trưng mạnh mẽ hơn, hoạt động như một dạng Ensemble learning.
* **Lưu ý:** Dropout chỉ dùng khi Training, không dùng khi Inference (dự đoán).

---

## 7. Ví dụ thực hành: Dự đoán giá nhà trên Kaggle

Phần cuối chương áp dụng toàn bộ kiến thức vào một bài toán thực tế: **Kaggle House Price Prediction**.
* **Quy trình:**
    1.  Tải và xử lý dữ liệu (xử lý missing values, one-hot encoding cho biến phân loại).
    2.  Xây dựng mô hình MLP.
    3.  Sử dụng **K-Fold Cross-Validation** để lựa chọn siêu tham số (Hyperparameters) tốt nhất.
    4.  Huấn luyện mô hình cuối cùng và xuất file nộp lên Kaggle.

---

### Tổng kết
Chương Multilayer Perceptrons là bước chuyển quan trọng từ Machine Learning cổ điển sang Deep Learning hiện đại. Việc nắm vững cách xây dựng các lớp ẩn, chọn hàm kích hoạt phù hợp và kiểm soát quá trình huấn luyện (tránh overfitting, ổn định gradient) là nền tảng cho các kiến trúc phức tạp hơn như CNN hay Transformer sau này.
